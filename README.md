# ğŸ’¼ ai-career-assistant-realworld

> **A production-grade RAG chatbot that answers questions about a professional profile
> with factual, cited answers â€” running 100% locally with no paid API required.**

[![CI](https://github.com/fredericoahb/ai-career-assistant-realworld/actions/workflows/ci.yml/badge.svg)](https://github.com/fredericoahb/ai-career-assistant-realworld/actions/workflows/ci.yml)
[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.111-009688.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/YOUR_USERNAME/ai-career-assistant-realworld?quickstart=1)

---

## Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Tech Stack & Design Decisions](#tech-stack--design-decisions)
- [Quick Start (Docker â€” 1 command)](#quick-start-docker--1-command)
- [Manual Setup (Local Dev)](#manual-setup-local-dev)
- [API Reference](#api-reference)
- [Environment Variables](#environment-variables)
- [Switching LLM Providers](#switching-llm-providers)
- [Switching Vector Store (DEV â†’ PROD)](#switching-vector-store-dev--prod)
- [Running Tests](#running-tests)
- [Project Structure](#project-structure)
- [Free Deployment Options](#free-deployment-options)
- [Contributing](#contributing)

---

## Features

| Category | Details |
|---|---|
| **RAG Pipeline** | Chunk â†’ Embed â†’ Store â†’ Retrieve â†’ Context â†’ LLM â†’ Cited answer |
| **100% Local LLM** | Ollama (llama3 default) â€” no API key, no cost |
| **Local Embeddings** | `all-MiniLM-L6-v2` via sentence-transformers |
| **Strict Mode** | Refuses answers when no evidence exists in the knowledge base |
| **Citations** | Every claim is annotated with `[Source N]` and the exact source document/section |
| **Two Vector Stores** | DEV: SQLite + FAISS &nbsp;/&nbsp; PROD: Postgres + pgvector |
| **Auth & RBAC** | JWT, admin role can ingest docs, user role can only chat |
| **Admin Panel** | Streamlit UI for uploading, listing, and deleting documents |
| **RealWorld-inspired API** | User registration/login, profile updates, tags â€” mirroring the [RealWorld spec](https://github.com/gothinkster/realworld) |
| **Observability** | Structured JSON logs (structlog), `/health` endpoint |
| **CI/CD** | GitHub Actions: lint â†’ unit tests â†’ integration tests â†’ Docker build â†’ Trivy scan |
| **One-command start** | `docker compose up --build` |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Docker Compose                            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   REST/JSON   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Streamlit  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚       FastAPI Backend         â”‚ â”‚
â”‚  â”‚  (Chat UI + â”‚               â”‚                               â”‚ â”‚
â”‚  â”‚  Admin UI)  â”‚               â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚  â”‚  Auth  â”‚  â”‚   Ingest    â”‚ â”‚ â”‚
â”‚                                â”‚  â”‚ (JWT)  â”‚  â”‚  (admin)    â”‚ â”‚ â”‚
â”‚                                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚                                â”‚                     â”‚        â”‚ â”‚
â”‚                                â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚                                â”‚         â”‚   RAG Pipeline   â”‚ â”‚ â”‚
â”‚                                â”‚         â”‚                  â”‚ â”‚ â”‚
â”‚                                â”‚         â”‚ 1. Chunker       â”‚ â”‚ â”‚
â”‚                                â”‚         â”‚ 2. Embedder      â”‚ â”‚ â”‚
â”‚                                â”‚         â”‚    (local model) â”‚ â”‚ â”‚
â”‚                                â”‚         â”‚ 3. Vector Store  â”‚ â”‚ â”‚
â”‚                                â”‚         â”‚    DEV:  FAISS   â”‚ â”‚ â”‚
â”‚                                â”‚         â”‚    PROD: pgvec   â”‚ â”‚ â”‚
â”‚                                â”‚         â”‚ 4. Retriever     â”‚ â”‚ â”‚
â”‚                                â”‚         â”‚ 5. LLM Client    â”‚ â”‚ â”‚
â”‚                                â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                   â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   SQLite /   â”‚â—„â”€â”€â”€â”€â”€ ORM â”€â”€â”€â”€â”€â”€â”€â”€â”‚      SQLAlchemy       â”‚  â”‚
â”‚  â”‚  PostgreSQL  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚    Ollama    â”‚â—„â”€â”€â”€â”€ HTTP /api/chat â”€â”€â”€â”€ LLM Client          â”‚
â”‚  â”‚  (llama3)    â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG Flow (per query)

```
User question
     â”‚
     â–¼
embed_query()          â† sentence-transformers, local, no API
     â”‚
     â–¼
vector_store.search()  â† FAISS (DEV) or pgvector (PROD)
     â”‚
     â–¼
filter by SIMILARITY_THRESHOLD
     â”‚ no results + STRICT_MODE=true
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º "No evidence found" (safe refusal)
     â”‚
     â–¼ results found
assemble context block  â† "[Source N] (filename Â§ Section)\n<chunk text>"
     â”‚
     â–¼
LLM.complete(system_prompt, context + question)
     â”‚
     â–¼
Answer with inline citations [Source 1], [Source 2]â€¦
     â”‚
     â–¼
Return to client with full citation metadata
```

---

## Tech Stack & Design Decisions

### Why FastAPI (Python) over ASP.NET Core?

| Reason | Detail |
|---|---|
| **ML ecosystem** | sentence-transformers, FAISS, LangChain, ollama-python are all Python-native. Using .NET would require subprocess calls or HTTP proxies, adding unnecessary complexity. |
| **Async-first** | FastAPI's `async/await` model maps naturally to I/O-heavy RAG workloads (vector search + LLM calls). |
| **Dev velocity** | Pydantic v2 provides excellent schema validation, serialization, and OpenAPI generation out of the box â€” ideal for a portfolio project that needs readable docs. |
| **Community** | FastAPI + Python has become the de facto standard for AI/ML API backends (2024-2025). Recruiters evaluating AI projects expect to see Python. |

### Why Streamlit over Next.js?

Streamlit was chosen because:
- **Zero JavaScript required** â€” keeps the codebase homogeneous (Python end-to-end).
- **Free deployment** on Hugging Face Spaces with `streamlit` runtime.
- **Rapid prototyping** â€” ideal for a portfolio demo where UI polish is secondary to backend sophistication.

If you prefer a React frontend, the FastAPI backend is a drop-in REST API â€” replace `frontend/` with any framework.

---

## Quick Start (Docker â€” 1 command)

### Prerequisites

- [Docker](https://docs.docker.com/get-docker/) â‰¥ 24
- [Docker Compose](https://docs.docker.com/compose/install/) v2 (ships with Docker Desktop)
- ~8 GB RAM free (for Ollama + llama3)
- ~8 GB disk (for the llama3 model)

### Steps

```bash
# 1. Clone the repository
git clone https://github.com/YOUR_USERNAME/ai-career-assistant-realworld.git
cd ai-career-assistant-realworld

# 2. Create your .env from the example
cp .env.example .env
# Edit .env and set a strong SECRET_KEY:
#   python -c "import secrets; print(secrets.token_hex(32))"

# 3. Start everything (Ollama will auto-pull llama3 on first run â€” ~4 GB)
docker compose up --build

# 4. Wait for services to be healthy (~2-5 min on first run)
#    Watch the logs: docker compose logs -f ollama

# 5. Open the app
#    Frontend (chat):  http://localhost:8501
#    Backend API docs: http://localhost:8000/docs
#    Health check:     http://localhost:8000/health
```

### Seed the knowledge base

```bash
# Register an admin account (or use the Streamlit UI)
curl -X POST http://localhost:8000/api/users \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","email":"admin@example.com","password":"admin123!"}'

# Login and capture the token
TOKEN=$(curl -s -X POST http://localhost:8000/api/users/login \
  -H "Content-Type: application/json" \
  -d '{"email":"admin@example.com","password":"admin123!"}' \
  | python3 -c "import sys,json; print(json.load(sys.stdin)['token'])")

# Promote to admin (one-time DB edit via the backend container)
docker compose exec backend python -c "
import asyncio
from app.models.database import AsyncSessionLocal
from app.models.db import User
from sqlalchemy import update

async def promote():
    async with AsyncSessionLocal() as s:
        await s.execute(update(User).where(User.username=='admin').values(is_admin=True))
        await s.commit()
        print('admin promoted')

asyncio.run(promote())
"

# Re-login to get a token with is_admin=true
TOKEN=$(curl -s -X POST http://localhost:8000/api/users/login \
  -H "Content-Type: application/json" \
  -d '{"email":"admin@example.com","password":"admin123!"}' \
  | python3 -c "import sys,json; print(json.load(sys.stdin)['token'])")

# Ingest the sample CV
curl -X POST http://localhost:8000/api/ingest \
  -H "Authorization: Bearer $TOKEN" \
  -F "file=@data/sample_cv.md"
```

Now open http://localhost:8501, register a regular user, and start chatting!

---

## Manual Setup (Local Dev)

```bash
# â”€â”€ Backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cd backend
python -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Copy and configure env
cp ../.env.example ../.env
# Set VECTOR_STORE_MODE=dev, OLLAMA_BASE_URL=http://localhost:11434

# Start Ollama separately (install from https://ollama.com)
ollama serve &
ollama pull llama3

# Run backend
uvicorn app.main:app --reload --port 8000

# â”€â”€ Frontend (new terminal) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cd frontend
pip install -r requirements.txt
API_BASE_URL=http://localhost:8000 streamlit run app.py --server.port 8501
```

---

## API Reference

Full interactive docs available at **http://localhost:8000/docs** (Swagger UI).

### Authentication

| Method | Path | Auth | Description |
|---|---|---|---|
| `POST` | `/api/users` | âœ— | Register new user |
| `POST` | `/api/users/login` | âœ— | Login, returns JWT |
| `GET` | `/api/users/me` | âœ“ | Get current user profile |
| `PUT` | `/api/users/me` | âœ“ | Update bio/avatar |

### Chat (RAG)

| Method | Path | Auth | Description |
|---|---|---|---|
| `POST` | `/api/chat` | âœ“ | Ask a question; returns cited answer |
| `GET` | `/api/chat/sessions/{id}/history` | âœ“ | Get full conversation history |

#### `POST /api/chat` â€” Example

```json
// Request
{
  "question": "What cloud certifications does the candidate hold?",
  "session_id": null
}

// Response
{
  "answer": "The candidate holds three cloud certifications: Google Professional Cloud Architect (2023) [Source 1], Certified Kubernetes Administrator (CKA) from CNCF (2022) [Source 2], and AWS Solutions Architect â€“ Associate (2020) [Source 3].",
  "citations": [
    { "index": 1, "source_label": "sample_cv.md Â§ Certifications", "excerpt": "Google Professional Cloud Architect (2023)..." },
    { "index": 2, "source_label": "sample_cv.md Â§ Certifications", "excerpt": "Certified Kubernetes Administrator (CKA)..." },
    { "index": 3, "source_label": "sample_cv.md Â§ Certifications", "excerpt": "AWS Solutions Architect â€“ Associate (2020)..." }
  ],
  "has_evidence": true,
  "session_id": 1
}
```

### Ingest (Admin only)

| Method | Path | Auth | Description |
|---|---|---|---|
| `POST` | `/api/ingest` | Admin | Upload and index a document |
| `GET` | `/api/ingest` | Admin | List all ingested documents |
| `DELETE` | `/api/ingest/{id}` | Admin | Remove document and its vectors |

### Utility

| Method | Path | Auth | Description |
|---|---|---|---|
| `GET` | `/health` | âœ— | System health check |
| `GET` | `/api/tags` | âœ— | List available tags |

---

## Environment Variables

See [`.env.example`](.env.example) for the full list with comments.

Key variables:

| Variable | Default | Description |
|---|---|---|
| `SECRET_KEY` | âš ï¸ **CHANGE ME** | JWT signing secret |
| `VECTOR_STORE_MODE` | `dev` | `dev` (SQLite+FAISS) or `prod` (Postgres+pgvector) |
| `LLM_PROVIDER` | `ollama` | `ollama` \| `openai` \| `anthropic` |
| `OLLAMA_MODEL` | `llama3` | Any model available in your Ollama instance |
| `STRICT_MODE` | `true` | Refuse answers without evidence |
| `SIMILARITY_THRESHOLD` | `0.30` | Minimum cosine similarity to use a chunk |
| `CHUNK_SIZE` | `400` | Tokens per chunk |

---

## Switching LLM Providers

The LLM is swappable at runtime via environment variables â€” **no code changes needed**.

```bash
# Use OpenAI GPT-4o-mini instead of local Ollama
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini

# Use Anthropic Claude Haiku
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-haiku-20240307
```

Install the optional providers:
```bash
pip install openai          # for OpenAI
pip install anthropic       # for Anthropic
```

---

## Switching Vector Store (DEV â†’ PROD)

```bash
# 1. Start Postgres with pgvector
docker compose -f docker-compose.prod.yml up postgres -d

# 2. Update .env
VECTOR_STORE_MODE=prod
POSTGRES_DSN=postgresql+asyncpg://career:secret@localhost:5432/career_db

# 3. Restart backend â€” tables and indexes are auto-created
docker compose -f docker-compose.prod.yml up --build backend
```

The `VectorStore` interface is identical for both adapters â€” callers never see the difference.

---

## Running Tests

```bash
cd backend
pip install -r requirements.txt
pytest -v
```

```
tests/test_chunker.py      â† Unit tests: chunking, dedup, section parsing
tests/test_integration.py  â† Integration tests: /auth, /ingest, /chat, /health
```

To run with coverage:
```bash
pip install pytest-cov
pytest --cov=app --cov-report=term-missing
```

---

## Project Structure

```
ai-career-assistant-realworld/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app factory + lifespan
â”‚   â”‚   â”œâ”€â”€ config.py               # Pydantic settings (all env vars)
â”‚   â”‚   â”œâ”€â”€ observability.py        # Structured logging (structlog)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”‚   â”œâ”€â”€ service.py          # JWT creation/verification, bcrypt
â”‚   â”‚   â”‚   â”œâ”€â”€ dependencies.py     # FastAPI deps: get_current_user, require_admin
â”‚   â”‚   â”‚   â””â”€â”€ router.py           # /api/users endpoints (RealWorld-inspired)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ chunker.py          # Markdown-aware sliding-window chunker + dedup
â”‚   â”‚   â”‚   â”œâ”€â”€ embedder.py         # sentence-transformers (local, no API)
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py     # FAISS (DEV) + pgvector (PROD) adapters
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py              # Ollama / OpenAI / Anthropic clients
â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py         # Full RAG flow: query â†’ cited answer
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py             # POST /api/chat â€” RAG Q&A
â”‚   â”‚   â”‚   â”œâ”€â”€ ingest.py           # POST /api/ingest â€” admin doc upload
â”‚   â”‚   â”‚   â””â”€â”€ health.py           # GET /health, GET /api/tags
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â”œâ”€â”€ db.py               # SQLAlchemy ORM: User, Document, Chunk, Chat
â”‚   â”‚       â””â”€â”€ database.py         # Engine factory (SQLite or Postgres)
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_chunker.py         # Unit tests for RAG chunker
â”‚   â”‚   â””â”€â”€ test_integration.py     # Integration tests (httpx + mocked RAG)
â”‚   â”‚
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ pytest.ini
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                      # Streamlit chat UI (main page)
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ 1_Admin.py              # Admin panel: upload / list / delete docs
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_cv.md                # Fictional CV for demo (no real PII)
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                  # lint â†’ test â†’ docker build â†’ trivy scan
â”‚
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json           # GitHub Codespaces configuration
â”‚
â”œâ”€â”€ docker-compose.yml              # DEV stack (SQLite + FAISS + Ollama)
â”œâ”€â”€ docker-compose.prod.yml         # PROD stack (Postgres + pgvector + Ollama)
â”œâ”€â”€ .env.example                    # Template for environment variables
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## Free Deployment Options

> âš ï¸ **Note:** Free tiers have resource limits that may vary or change. The options
> below are provided as starting points; always check the provider's current terms.

### Option A â€” Hugging Face Spaces (Frontend)

Hugging Face Spaces supports Streamlit apps for free with a CPU tier.

1. Create a new Space at https://huggingface.co/new-space
2. Select **Streamlit** runtime
3. Push `frontend/` contents to the Space repository
4. Add a Space secret: `API_BASE_URL=https://your-backend-url`

**Limitation:** The free CPU tier has limited RAM and will sleep after inactivity.

### Option B â€” Railway (Backend)

[Railway](https://railway.app) offers a free hobby tier (~$5 credit/month).

```bash
# Install Railway CLI
npm install -g @railway/cli

# Deploy
railway login
railway init
railway up --service backend
railway variables set --from-file .env
```

### Option C â€” Render (Backend)

[Render](https://render.com) has a free tier for web services (spins down on inactivity).

1. Connect your GitHub repo
2. Set root directory to `backend/`
3. Build command: `pip install -r requirements.txt`
4. Start command: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`
5. Add environment variables from `.env.example`

### Option D â€” Local + Ngrok (Quick Demo)

For a quick demo without deploying:

```bash
# Expose local backend
ngrok http 8000
# Use the ngrok URL as API_BASE_URL in the Streamlit app
```

### Ollama on free tier

Running a local LLM on free cloud tiers is generally not feasible due to RAM constraints.
When deploying to the cloud, switch to an API provider:

```
LLM_PROVIDER=openai
OPENAI_API_KEY=your-key
```

OpenAI's `gpt-4o-mini` costs approximately $0.15 per 1M input tokens â€” very low cost
for a portfolio demo with light traffic.

---

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch: `git checkout -b feat/your-feature`
3. Add tests for any new logic
4. Run `ruff check . && black . && pytest` before pushing
5. Open a pull request

---

## License

MIT â€” see [LICENSE](LICENSE).

---

*Built with â¤ï¸ to demonstrate real-world RAG engineering practices.*
